{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 4\n",
    "## Universidad del Valle de Guatemala <br> Facultad de Ingeniería\n",
    "#### Departamento de Ciencias de la Computación <br> Data Science - Sección 10 <br> Grupo 12\n",
    "#### Cristian Laynez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/cristian/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/cristian/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/cristian/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Cargar el archivo de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Ya se descargo el archivo train.csv\n",
    "data_frame = pd.read_csv('./data/train.csv')\n",
    "data_frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede apreciar que se tienen las siguientes variables junto con su descripción y estado:\n",
    "\n",
    "- id: Este sería el identificador del texto\n",
    "- keyword: Palabra clave de como \"catagolaron\" el tweet\n",
    "- location: El lugar en donde se publico el tewt\n",
    "- text: El texto del mismo tweet publicado\n",
    "- target: Clasificacion del tweet si es un desastre real o no\n",
    "\n",
    "Se puede apreciar que se pueden encontrar datos vacios en keyword y en location."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Limpie y preprocese los datos (Describir de forma detallada las actividades de preprocesamiento que se llevó a cabo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stemmed_text(text):    \n",
    "    stemmer = PorterStemmer()\n",
    "    words = word_tokenize(text)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lemmatization(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir el texto en minúsculas\n",
    "data_frame['text'] = data_frame['text'].str.lower()\n",
    "\n",
    "# Quitar caracteres especiales “#”,”@” o los apóstrofes\n",
    "data_frame['text'] = data_frame['text'].str.replace(r'[#@\\'\"]', '', regex=True)\n",
    "\n",
    "# Quitar urls\n",
    "data_frame['text'] = data_frame['text'].str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', regex=True)\n",
    "\n",
    "# Eliminar signos de puntuacion\n",
    "data_frame['text'] = data_frame['text'].str.replace(r'[^\\w\\s]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisar si hay emojis en el texto y luego quitarlos\n",
    "data_frame['text'] = data_frame['text'].apply(remove_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quitar los artículos, preposiciones y conjunciones (stopwords)\n",
    "data_frame['text'] = data_frame['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar las palabras para tratar de convertir todas las palabras encontradas en solo una palabra\n",
    "data_frame['text'] = data_frame['text'].apply(get_stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar la función de lematización al atributo 'text' para identificar la parte\n",
    "# del discurso de una palabra determinada y luego aplicando reglas más complejas \n",
    "# para transformar la palabra en su verdadera raíz\n",
    "data_frame['text'] = data_frame['text'].apply(apply_lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deed reason earthquak may allah forgiv u</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 peopl receiv wildfir evacu order california</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>two giant crane hold bridg collaps nearbi home</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aria_ahrari thetawniest control wild fire cali...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m194 0104 utc5km volcano hawaii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>polic investig ebik collid car littl portug eb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>latest home raze northern california wildfir a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0              deed reason earthquak may allah forgiv u       1  \n",
       "1                  forest fire near la rong sask canada       1  \n",
       "2     resid ask shelter place notifi offic evacu she...       1  \n",
       "3     13000 peopl receiv wildfir evacu order california       1  \n",
       "4     got sent photo rubi alaska smoke wildfir pour ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608     two giant crane hold bridg collaps nearbi home       1  \n",
       "7609  aria_ahrari thetawniest control wild fire cali...       1  \n",
       "7610                    m194 0104 utc5km volcano hawaii       1  \n",
       "7611  polic investig ebik collid car littl portug eb...       1  \n",
       "7612  latest home raze northern california wildfir a...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se podrá observar se opto por quitar convertir todo el te xto en minusculas, en quitar caracteres especiales, removemoer urls, eliminar signos de puntuacion extraños, luego quitar todos los emojis y luego remover los \"stopwords\" el cual en consecuencia nos permite tener data mejor procesada. Se tomara en cuetna el tipo de keywrod y location que tiene para apoyarnos en detectar las palabras usadas con más frecuencia y aparte detectar si en efecto se hablan de desastres reales o no."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Obtener frecuencia de las palabras tanto de los tweets de desastres como de los que no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar los tweets de desastres y los que no lo son\n",
    "disasters_tweets = data_frame[data_frame['target'] == 1]['text']\n",
    "no_disasters_tweets = data_frame[data_frame['target'] == 0]['text']\n",
    "\n",
    "# Tokenizar los tweets\n",
    "tokens_disasters = word_tokenize(' '.join(disasters_tweets))\n",
    "tokens_no_disasters = word_tokenize(' '.join(no_disasters_tweets))\n",
    "\n",
    "# Calcular la frecuencia de las palabras\n",
    "freq_disasters = FreqDist(tokens_disasters)\n",
    "freq_no_disasters = FreqDist(tokens_no_disasters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras más comunes en tweets de desastres:\n",
      "fire: 267\n",
      "bomb: 180\n",
      "kill: 158\n",
      "news: 136\n",
      "via: 121\n",
      "flood: 120\n",
      "disast: 117\n",
      "california: 115\n",
      "crash: 112\n",
      "suicid: 110\n",
      "train: 109\n",
      "polic: 107\n",
      "peopl: 106\n",
      "amp: 106\n",
      "famili: 105\n",
      "u: 104\n",
      "attack: 104\n",
      "evacu: 101\n",
      "like: 101\n",
      "home: 100\n"
     ]
    }
   ],
   "source": [
    "# Imprimir las palabras más comunes\n",
    "print('Palabras más comunes en tweets de desastres:')\n",
    "for word, frequency in freq_disasters.most_common(20):\n",
    "    print(f'{word}: {frequency}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Palabras más comunes en tweets que no son de desastres:\n",
      "like: 307\n",
      "im: 244\n",
      "get: 223\n",
      "amp: 192\n",
      "new: 168\n",
      "u: 142\n",
      "go: 142\n",
      "dont: 141\n",
      "one: 135\n",
      "love: 116\n",
      "bodi: 116\n",
      "bag: 109\n",
      "time: 104\n",
      "video: 102\n",
      "via: 99\n",
      "want: 98\n",
      "see: 98\n",
      "scream: 98\n",
      "would: 97\n",
      "make: 97\n"
     ]
    }
   ],
   "source": [
    "print('\\nPalabras más comunes en tweets que no son de desastres:')\n",
    "for word, frequency in freq_no_disasters.most_common(20):\n",
    "    print(f'{word}: {frequency}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ¿Qué palabras cree que le servirán para hacer un mejor modelo de clasificación?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso de tweets de desastres:\n",
    "- fire\n",
    "- bomb\n",
    "- kill\n",
    "- news\n",
    "- suicide\n",
    "- police\n",
    "- flood\n",
    "- disast\n",
    "- amp\n",
    "- attack\n",
    "- crash\n",
    "- polic\n",
    "- evacu\n",
    "\n",
    "En lo personal creo que estas serian las mejores palabras para clasificar un desastre real ya que estos tocan temas serios."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso de tweets de no desastres:\n",
    "- like\n",
    "- get\n",
    "- amp\n",
    "- go\n",
    "- dont\n",
    "- one\n",
    "- love\n",
    "- bodi\n",
    "- time\n",
    "- video\n",
    "- via\n",
    "- want\n",
    "- see\n",
    "- would\n",
    "\n",
    "Para el caso de los no desastres considero que estas palabras serían mejores para clasificar los temas de ironia y comedia, ya que como se puede apreciar son palabras haciendo referencia a una persona, a algunos temas agradables e inclusive hablando sobre \"que pasaría\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ¿Vale la pena explorar bigramas o trigramas para analizar contexto?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vamos a obtener y crear los bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear bigramas\n",
    "bigrams_disasters = list(bigrams(tokens_disasters))\n",
    "bigrams_no_disasters = list(bigrams(tokens_no_disasters))\n",
    "\n",
    "# Calcular la frecuencia de los bigramas\n",
    "freq_bigrams_disasters = FreqDist(bigrams_disasters)\n",
    "freq_bigrams_no_disasters = FreqDist(bigrams_no_disasters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigramas más comunes en tweets de desastres:\n",
      "('suicid', 'bomber'): 60\n",
      "('atom', 'bomb'): 50\n",
      "('train', 'derail'): 49\n",
      "('mass', 'murder'): 45\n",
      "('northern', 'california'): 41\n",
      "('oil', 'spill'): 38\n",
      "('suicid', 'bomb'): 36\n",
      "('california', 'wildfir'): 36\n",
      "('burn', 'build'): 35\n",
      "('bomber', 'deton'): 31\n",
      "('70', 'year'): 30\n",
      "('confirm', 'mh370'): 29\n",
      "('deton', 'bomb'): 29\n",
      "('home', 'raze'): 29\n",
      "('sever', 'thunderstorm'): 28\n"
     ]
    }
   ],
   "source": [
    "# Imprimir los bigramas más comunes\n",
    "print('Bigramas más comunes en tweets de desastres:')\n",
    "for word, frequency in freq_bigrams_disasters.most_common(15):\n",
    "    print(f'{word}: {frequency}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bigramas más comunes en tweets que no son de desastres:\n",
      "('bodi', 'bag'): 70\n",
      "('cross', 'bodi'): 38\n",
      "('look', 'like'): 36\n",
      "('youtub', 'video'): 36\n",
      "('like', 'youtub'): 35\n",
      "('gon', 'na'): 32\n",
      "('wan', 'na'): 30\n",
      "('feel', 'like'): 27\n",
      "('full', 'reû_'): 25\n",
      "('burn', 'build'): 23\n",
      "('full', 'read'): 22\n",
      "('reddit', 'quarantin'): 21\n",
      "('content', 'polici'): 20\n",
      "('emerg', 'servic'): 18\n",
      "('via', 'youtub'): 18\n"
     ]
    }
   ],
   "source": [
    "print('\\nBigramas más comunes en tweets que no son de desastres:')\n",
    "for word, frequency in freq_bigrams_no_disasters.most_common(15):\n",
    "    print(f'{word}: {frequency}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ahora vamos a crear trigramas para analizar los diferentes contextos, se utilizaran 3 n-gramas para calcular los trigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear trigramas\n",
    "trigrams_disasters = list(ngrams(tokens_disasters, 3))\n",
    "trigrams_no_disasters = list(ngrams(tokens_no_disasters, 3))\n",
    "\n",
    "# Calcular la frecuencia de los trigramas\n",
    "freq_trigrams_disasters = FreqDist(trigrams_disasters)\n",
    "freq_trigrams_no_disasters = FreqDist(trigrams_no_disasters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigramas más comunes en tweets de desastres:\n",
      "('suicid', 'bomber', 'deton'): 31\n",
      "('northern', 'california', 'wildfir'): 29\n",
      "('latest', 'home', 'raze'): 28\n",
      "('home', 'raze', 'northern'): 28\n",
      "('pkk', 'suicid', 'bomber'): 28\n",
      "('bomber', 'deton', 'bomb'): 28\n",
      "('raze', 'northern', 'california'): 27\n",
      "('16yr', 'old', 'pkk'): 27\n",
      "('old', 'pkk', 'suicid'): 27\n",
      "('famili', 'sue', 'legionnair'): 26\n",
      "('40', 'famili', 'affect'): 26\n",
      "('famili', 'affect', 'fatal'): 26\n",
      "('affect', 'fatal', 'outbreak'): 26\n",
      "('obama', 'declar', 'disast'): 25\n",
      "('declar', 'disast', 'typhoondevast'): 25\n"
     ]
    }
   ],
   "source": [
    "# Imprimir los trigramas más comunes\n",
    "print('Trigramas más comunes en tweets de desastres:')\n",
    "for word, frequency in freq_trigrams_disasters.most_common(15):\n",
    "    print(f'{word}: {frequency}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trigramas más comunes en tweets que no son de desastres:\n",
      "('like', 'youtub', 'video'): 35\n",
      "('cross', 'bodi', 'bag'): 22\n",
      "('reddit', 'quarantin', 'offens'): 18\n",
      "('quarantin', 'offens', 'content'): 18\n",
      "('pick', 'fan', 'armi'): 17\n",
      "('reddit', 'new', 'content'): 16\n",
      "('new', 'content', 'polici'): 16\n",
      "('stock', 'market', 'crash'): 16\n",
      "('full', 'read', 'ebay'): 15\n",
      "('ignit', 'knock', 'deton'): 15\n",
      "('content', 'polici', 'goe'): 15\n",
      "('polici', 'goe', 'effect'): 15\n",
      "('goe', 'effect', 'mani'): 15\n",
      "('effect', 'mani', 'horribl'): 15\n",
      "('mani', 'horribl', 'subreddit'): 15\n"
     ]
    }
   ],
   "source": [
    "print('\\nTrigramas más comunes en tweets que no son de desastres:')\n",
    "for word, frequency in freq_trigrams_no_disasters.most_common(15):\n",
    "    print(f'{word}: {frequency}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En base a los resultados obtenidos se puede apreciar que vale más la pena utilizar los trigramas ya que estos suelen dar mucho más contexto que los bigramas, los bigramas algunos de sus resultados pueden dejar muy ambiguos el contexto y se puede interpretar de formas muy diferentes. Sin emmbargo con los trigramas se combinan palabras más serias y dan más contexto ya sea un desastre real o no."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modelo de predicción"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este escenario se creara un modelo de predicción con la ayuda y el apoyo del algoritmo SVM (Support Vector Machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cristian/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/home/cristian/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/home/cristian/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/home/cristian/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.001: 0.7342436974789915\n",
      "Accuracy for C=0.005: 0.7804621848739496\n",
      "Accuracy for C=0.01: 0.7941176470588235\n",
      "Accuracy for C=0.05: 0.7983193277310925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cristian/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.1: 0.7967436974789915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cristian/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.2: 0.7951680672268907\n"
     ]
    }
   ],
   "source": [
    "# Crear el vectorizador\n",
    "ngram_vectorizer = CountVectorizer(\n",
    "    binary=True, \n",
    "    ngram_range=(1, 3), \n",
    "    stop_words=['in', 'of', 'at', 'a', 'the']\n",
    ")\n",
    "\n",
    "# Aplicar el vectorizador a los datos de texto\n",
    "X = ngram_vectorizer.fit_transform(data_frame['text'])\n",
    "\n",
    "# Definir la variable objetivo\n",
    "y = data_frame['target']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size = 0.75)\n",
    "\n",
    "# Entrenar el modelo SVM con diferentes valores de C y seleccionar el que da la mejor precisión\n",
    "for c in [0.001, 0.005, 0.01, 0.05, 0.1, 0.2]:\n",
    "    svm = LinearSVC(C=c)\n",
    "    svm.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" % (c, accuracy_score(y_val, svm.predict(X_val))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.9382634966504663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cristian/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo final con el mejor valor de C y calcular la precisión final\n",
    "final_svm = LinearSVC(C=0.01)\n",
    "final_svm.fit(X, y)\n",
    "print (\"Final Accuracy: %s\" % accuracy_score(y, final_svm.predict(X)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede apreciar al final se logro obtener un accurracy de 0.93, el cual es un resultado excelente y sobresaliente, esto fue gracias al preprocesamiento realizado en el texto con anterioridad. Gracias a la combinación de las técnicas anteriores se logro obtener un modelo capaz de realizar predicciones muy acertadas y eficaces."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Realizar un análisis exploratorio de los datos para entrenderlos mejor, documente todos los análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigar qué palabra se repite más en cada una de las categorías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer una nube de palabras para visualizar las que aparecen con más frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer un histograma con las palabras que más se repiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discutir sobre las palabras que tienen presencia en todas las categorías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...\n",
    "# ! explicar los cruces de variables, hay gráficos\n",
    "# ! explicativos y análisis que permiten comprender el conjunto de datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Elabore una función en el que el usuario ingrese un tweet y el sistema lo clasifica en desastre o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para clasificar los nuevos tweets a ingresar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
