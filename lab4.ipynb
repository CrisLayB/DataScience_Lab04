{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 4\n",
    "## Universidad del Valle de Guatemala <br> Facultad de Ingeniería\n",
    "#### Departamento de Ciencias de la Computación <br> Data Science - Sección 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/crislay/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/crislay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Cargar el archivo de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Ya se descargo el archivo train.csv\n",
    "data_frame = pd.read_csv('./data/train.csv')\n",
    "data_frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede apreciar que se tienen las siguientes variables junto con su descripción y estado:\n",
    "\n",
    "- id: Este sería el identificador del texto\n",
    "- keyword: Palabra clave de como \"catagolaron\" el tweet\n",
    "- location: El lugar en donde se publico el tewt\n",
    "- text: El texto del mismo tweet publicado\n",
    "- target: Clasificacion del tweet si es un desastre real o no\n",
    "\n",
    "Se puede apreciar que se pueden encontrar datos vacios en keyword y en location."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Limpie y preprocese los datos (Describir de forma detallada las actividades de preprocesamiento que se llevó a cabo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(['the', 'a', 'an', 'in', 'at', 'is', 'it', 'this', 'that', 'on', 'was', 'by', 'for', 'with', 'as', 'and', 'or', 'to', 'from', 'of'])\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir el texto en minúsculas\n",
    "data_frame['text'] = data_frame['text'].str.lower()\n",
    "\n",
    "# Quitar caracteres especiales “#”,”@” o los apóstrofes\n",
    "data_frame['text'] = data_frame['text'].str.replace(r'[#@\\'\"]', '', regex=True)\n",
    "\n",
    "# Quitar urls\n",
    "data_frame['text'] = data_frame['text'].str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', regex=True)\n",
    "\n",
    "# Eliminar signos de puntuacion\n",
    "data_frame['text'] = data_frame['text'].str.replace(r'[^\\w\\s]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisar si hay emojis en el texto y luego quitarlos\n",
    "data_frame['text'] = data_frame['text'].apply(remove_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quitar los artículos, preposiciones y conjunciones (stopwords)\n",
    "data_frame['text'] = data_frame['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are reason earthquake may allah forg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked shelter place are being no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent photo ruby alaska smoke wildfire...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>two giant cranes holding bridge collapse into ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aria_ahrary thetawniest out control wild fires...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m194 0104 utc5km s volcano hawaii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>police investigating after ebike collided car ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>latest more homes razed northern california wi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     our deeds are reason earthquake may allah forg...       1  \n",
       "1                 forest fire near la ronge sask canada       1  \n",
       "2     all residents asked shelter place are being no...       1  \n",
       "3     13000 people receive wildfires evacuation orde...       1  \n",
       "4     just got sent photo ruby alaska smoke wildfire...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  two giant cranes holding bridge collapse into ...       1  \n",
       "7609  aria_ahrary thetawniest out control wild fires...       1  \n",
       "7610                  m194 0104 utc5km s volcano hawaii       1  \n",
       "7611  police investigating after ebike collided car ...       1  \n",
       "7612  latest more homes razed northern california wi...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se podrá observar se opto por quitar convertir todo el te xto en minusculas, en quitar caracteres especiales, removemoer urls, eliminar signos de puntuacion extraños, luego quitar todos los emojis y luego remover los \"stopwords\" el cual en consecuencia nos permite tener data mejor procesada. Se tomara en cuetna el tipo de keywrod y location que tiene para apoyarnos en detectar las palabras usadas con más frecuencia y aparte detectar si en efecto se hablan de desastres reales o no."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Obtener frecuencia de las palabras tanto de los tweets de desastres como de los que no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar los tweets de desastres y los que no lo son\n",
    "disasters_tweets = data_frame[data_frame['target'] == 1]['text']\n",
    "no_disasters_tweets = data_frame[data_frame['target'] == 0]['text']\n",
    "\n",
    "# Tokenizar los tweets\n",
    "tokens_disasters = word_tokenize(' '.join(disasters_tweets))\n",
    "tokens_no_disasters = word_tokenize(' '.join(no_disasters_tweets))\n",
    "\n",
    "# Calcular la frecuencia de las palabras\n",
    "freq_disasters = FreqDist(tokens_disasters)\n",
    "freq_no_disasters = FreqDist(tokens_no_disasters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras más comunes en tweets de desastres:\n",
      "i: 294\n",
      "after: 181\n",
      "fire: 178\n",
      "are: 166\n",
      "have: 142\n",
      "news: 136\n",
      "my: 131\n",
      "you: 129\n",
      "over: 127\n",
      "up: 126\n",
      "more: 122\n",
      "via: 121\n",
      "were: 119\n",
      "disaster: 117\n",
      "be: 114\n",
      "california: 111\n",
      "suicide: 110\n",
      "no: 107\n",
      "police: 107\n",
      "amp: 106\n"
     ]
    }
   ],
   "source": [
    "# Imprimir las palabras más comunes\n",
    "print('Palabras más comunes en tweets de desastres:')\n",
    "for word, frequency in freq_disasters.most_common(20):\n",
    "    print(f'{word}: {frequency}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Palabras más comunes en tweets que no son de desastres:\n",
      "i: 1076\n",
      "you: 664\n",
      "my: 544\n",
      "be: 287\n",
      "me: 260\n",
      "like: 253\n",
      "im: 243\n",
      "have: 242\n",
      "so: 240\n",
      "are: 235\n",
      "your: 231\n",
      "just: 231\n",
      "but: 220\n",
      "not: 209\n",
      "up: 195\n",
      "out: 195\n",
      "amp: 192\n",
      "all: 185\n",
      "if: 183\n",
      "will: 179\n"
     ]
    }
   ],
   "source": [
    "print('\\nPalabras más comunes en tweets que no son de desastres:')\n",
    "for word, frequency in freq_no_disasters.most_common(20):\n",
    "    print(f'{word}: {frequency}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ¿Qué palabras cree que le servirán para hacer un mejor modelo de clasificación?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso de tweets de desastres:\n",
    "- after\n",
    "- fire\n",
    "- news\n",
    "- over\n",
    "- more\n",
    "- suicide\n",
    "- police\n",
    "- amp\n",
    "\n",
    "En lo personal creo que estas serian las mejores palabras para clasificar un desastre real ya que estos tocan temas serios."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso de tweets de no desastres:\n",
    "- my\n",
    "- be\n",
    "- me\n",
    "- like\n",
    "- your\n",
    "- just\n",
    "- if\n",
    "- will\n",
    "\n",
    "Para el caso de los no desastres considero que estas palabras serían mejores para clasificar los temas de ironia y comedia, ya que como se puede apreciar son palabras haciendo referencia a una persona, a algunos temas agradables e inclusive hablando sobre \"que pasaría\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ¿Vale la pena explorar bigramas o trigramas para analizar contexto?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vamos a obtener y crear los bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear bigramas\n",
    "bigrams_disasters = list(bigrams(tokens_disasters))\n",
    "bigrams_no_disasters = list(bigrams(tokens_no_disasters))\n",
    "\n",
    "# Calcular la frecuencia de los bigramas\n",
    "freq_bigrams_disasters = FreqDist(bigrams_disasters)\n",
    "freq_bigrams_no_disasters = FreqDist(bigrams_no_disasters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigramas más comunes en tweets de desastres:\n",
      "('suicide', 'bomber'): 59\n",
      "('more', 'than'): 43\n",
      "('have', 'been'): 42\n",
      "('northern', 'california'): 41\n",
      "('oil', 'spill'): 38\n",
      "('suicide', 'bombing'): 34\n",
      "('california', 'wildfire'): 34\n",
      "('more', 'homes'): 32\n",
      "('may', 'have'): 31\n",
      "('70', 'years'): 30\n",
      "('homes', 'razed'): 29\n",
      "('confirmed', 'mh370'): 28\n",
      "('latest', 'more'): 28\n",
      "('razed', 'northern'): 28\n",
      "('16yr', 'old'): 28\n"
     ]
    }
   ],
   "source": [
    "# Imprimir los bigramas más comunes\n",
    "print('Bigramas más comunes en tweets de desastres:')\n",
    "for word, frequency in freq_bigrams_disasters.most_common(15):\n",
    "    print(f'{word}: {frequency}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bigramas más comunes en tweets que no son de desastres:\n",
      "('i', 'just'): 55\n",
      "('if', 'you'): 48\n",
      "('i', 'have'): 44\n",
      "('i', 'dont'): 43\n",
      "('you', 'are'): 38\n",
      "('will', 'be'): 38\n",
      "('cross', 'body'): 38\n",
      "('youtube', 'video'): 36\n",
      "('i', 'liked'): 35\n",
      "('liked', 'youtube'): 35\n",
      "('do', 'you'): 34\n",
      "('i', 'cant'): 32\n",
      "('gon', 'na'): 32\n",
      "('wan', 'na'): 30\n",
      "('i', 'will'): 29\n"
     ]
    }
   ],
   "source": [
    "print('\\nBigramas más comunes en tweets que no son de desastres:')\n",
    "for word, frequency in freq_bigrams_no_disasters.most_common(15):\n",
    "    print(f'{word}: {frequency}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ahora vamos a crear trigramas para analizar los diferentes contextos, se utilizaran 3 n-gramas para calcular los trigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear trigramas\n",
    "trigrams_disasters = list(ngrams(tokens_disasters, 3))\n",
    "trigrams_no_disasters = list(ngrams(tokens_no_disasters, 3))\n",
    "\n",
    "# Calcular la frecuencia de los trigramas\n",
    "freq_trigrams_disasters = FreqDist(trigrams_disasters)\n",
    "freq_trigrams_no_disasters = FreqDist(trigrams_no_disasters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigramas más comunes en tweets de desastres:\n",
      "('northern', 'california', 'wildfire'): 29\n",
      "('more', 'homes', 'razed'): 29\n",
      "('latest', 'more', 'homes'): 28\n",
      "('homes', 'razed', 'northern'): 28\n",
      "('pkk', 'suicide', 'bomber'): 28\n",
      "('suicide', 'bomber', 'who'): 28\n",
      "('bomber', 'who', 'detonated'): 28\n",
      "('who', 'detonated', 'bomb'): 28\n",
      "('razed', 'northern', 'california'): 27\n",
      "('16yr', 'old', 'pkk'): 27\n",
      "('old', 'pkk', 'suicide'): 27\n",
      "('families', 'sue', 'over'): 26\n",
      "('sue', 'over', 'legionnaires'): 26\n",
      "('more', 'than', '40'): 26\n",
      "('than', '40', 'families'): 26\n"
     ]
    }
   ],
   "source": [
    "# Imprimir los trigramas más comunes\n",
    "print('Trigramas más comunes en tweets de desastres:')\n",
    "for word, frequency in freq_trigrams_disasters.most_common(15):\n",
    "    print(f'{word}: {frequency}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trigramas más comunes en tweets que no son de desastres:\n",
      "('i', 'liked', 'youtube'): 35\n",
      "('liked', 'youtube', 'video'): 35\n",
      "('reddit', 'will', 'now'): 21\n",
      "('will', 'now', 'quarantine'): 21\n",
      "('cross', 'body', 'bag'): 18\n",
      "('now', 'quarantine', 'offensive'): 18\n",
      "('quarantine', 'offensive', 'content'): 18\n",
      "('my', 'pick', 'fan'): 17\n",
      "('pick', 'fan', 'army'): 17\n",
      "('reddits', 'new', 'content'): 16\n",
      "('new', 'content', 'policy'): 16\n",
      "('stock', 'market', 'crash'): 16\n",
      "('full', 'read', 'ebay'): 15\n",
      "('fall', 'off', 'cliff'): 15\n",
      "('ignition', 'knock', 'detonation'): 15\n"
     ]
    }
   ],
   "source": [
    "print('\\nTrigramas más comunes en tweets que no son de desastres:')\n",
    "for word, frequency in freq_trigrams_no_disasters.most_common(15):\n",
    "    print(f'{word}: {frequency}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En base a los resultados obtenidos se puede apreciar que vale más la pena utilizar los trigramas ya que estos suelen dar mucho más contexto que los bigramas, los bigramas algunos de sus resultados pueden dejar muy ambiguos el contexto y se puede interpretar de formas muy diferentes. Sin emmbargo con los trigramas se combinan palabras más serias y dan más contexto ya sea un desastre real o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de predicción y definir el algoritmo a utilizar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Realizar un análisis exploratorio de los datos para entrenderlos mejor, documente todos los análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigar qué palabra se repite más en cada una de las categorías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer una nube de palabras para visualizar las que aparecen con más frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer un histograma con las palabras que más se repiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discutir sobre las palabras que tienen presencia en todas las categorías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...\n",
    "# ! explicar los cruces de variables, hay gráficos\n",
    "# ! explicativos y análisis que permiten comprender el conjunto de datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Elabore una función en el que el usuario ingrese un tweet y el sistema lo clasifica en desastre o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para clasificar los nuevos tweets a ingresar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
